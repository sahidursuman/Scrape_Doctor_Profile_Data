{
  "name": "Scrape doctor profile data",
  "tagline": "Use selenium to scrape doctor profile data from state's board of medicine website",
  "body": "### Scrape Doctor Profile Data from State's Board of Medicine Websites\r\n\r\nan example of using **selenium + lxml** libs to extract table data from web pages.\r\n\r\n### Background\r\n\r\nI'll use the Washington DC board of medicine website as an example: https://app.hpla.doh.dc.gov/Weblookup/\r\n![](https://github.com/siutanwong/Scrape_Doctor_Profile_Data/blob/master/img/dc.png)\r\n\r\nMy goal is to scrape all the search results (multiple pages) from the .asp website and then save the data into a csv file.\r\n![](https://github.com/siutanwong/Scrape_Doctor_Profile_Data/blob/master/img/table.png)\r\n\r\nFirst, let's set up our environment:\r\n\r\n```\r\n$ pip install selenium\r\n$ pip install lxml\r\n```\r\nThen we add the following libs in our script\r\n```\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.support.ui import Select\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom lxml import html\r\nimport csv\r\n```\r\n\r\n#### Loading the search page\r\n```\r\ndriver = webdriver.Firefox()\r\ndriver.get(\"https://app.hpla.doh.dc.gov/Weblookup/\")\r\n```\r\n#### Selecting from the \"License Type\" dropdown menu\r\n![](https://github.com/siutanwong/Scrape_Doctor_Profile_Data/blob/master/img/table.png)\r\nIf you look at the HTML associated with this dropdown menu, you will see the ```name``` that attributes to it is ```t_web_lookup__license_type_name```\r\nNext, select \"Medicine and Surgery\" from the dropdown and click the \"Inspect Element\", you will find the value associated with it is ```\"MEDICINE AND SURGERY\"```.\r\n\r\nLet's add these attributions in our script:\r\n```\r\ndropdown = Select(driver.find_element_by_name(\"t_web_lookup__license_type_name\"))\r\ndropdown.select_by_value(\"MEDICINE AND SURGERY\")\r\n```\r\n#### Click the Search button\r\nLike the dropdown, we need to find the HTML of the search button on the search page, here it is in the web inspector:\r\n![](https://github.com/siutanwong/Scrape_Doctor_Profile_Data/blob/master/img/page.png)\r\n\r\nWe can select this button and add a .click function in our script:\r\n```\r\nsearch_button = driver.find_element_by_id(\"sch_button\")\r\nsearch_button.click()\r\n```\r\nNow when you run our script, a Firefox broswer will pop out and submit the form to the server.\r\n\r\n#### Extracting the search results\r\nLet's create a function called ```get_data()``` and use the ```html``` function from ```lxml``` to extract the table data (ps: you can also use beautifulsoup if you like, I like lxml better because it is more straight forward.)\r\n```\r\ndef get_data(source):\r\n    texts = []\r\n    content = html.fromstring(source)\r\n    rows - content.xpath(\".//table[id@='datagrid_results']//tr\")\r\n    for row in rows:\r\n        columns = row.xpath(\".//td\")\r\n        text = [col.text_content() for col in columns]\r\n        texts.append(text)\r\n    return texts\r\n```\r\nThis function will extract all the table dat on the current page, and put it in the list ```texts```.\r\n\r\n#### Pagination\r\nWhen you scoll down to the bottom of the page, you will notice that it only shows 40 pages at a time:\r\n![](.//img/page.png)\r\nAnd when you click the \"...\" next to page 40, it will take to page 41 - 80, and so on so forth.\r\nIn order to get all the reuslts, we need to find the ```xpath``` of each page  number link, starting with page2:\r\n```\r\ndef go_to_next_page():\r\n    next_page_link = driver.find_element_by_xpath(\"//table[@id='datagrid_results']//tr//td//span/following-sibling::a\")\r\n    next_page_link.click()\r\n```\r\nThis function allows the script to go through all the ```<a>```tags that contain the page number link.\r\nNow we have all the stuff we need to extract data from this database, all we need to do write the data into a csv file:\r\n```\r\nWith open(\"dc.csv\", \"wb\") as output:\r\n    writer = csv.writer(output, delimiter=\",\")\r\n    While True:\r\n        data_from_page = get_data(driver.page_source)\r\n        writer.writerows(data_from_page)\r\n        try:\r\n            go_to_next_page()\r\n        except:\r\n            break\r\ndriver.close()\r\n```\r\n####Conclusion\r\nThat's all! Run the script on your computer and go get yourself a cup of coffee :coffee:\r\nBy the time you come back, the data will be ready for you in a csv file!\r\n\r\n####Contact\r\nIf you have questions regarding the script or scraping in general, feel free to shoot me an email at wongsiutan@gmail.com\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}