<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Scrape doctor profile data by siutanwong</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Scrape doctor profile data</h1>
      <h2 class="project-tagline">Use selenium to scrape doctor profile data from state&#39;s board of medicine website</h2>
      <a href="https://github.com/siutanwong/Scrape_Doctor_Profile_Data" class="btn">View on GitHub</a>
      <a href="https://github.com/siutanwong/Scrape_Doctor_Profile_Data/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/siutanwong/Scrape_Doctor_Profile_Data/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="scrape-doctor-profile-data-from-states-board-of-medicine-websites" class="anchor" href="#scrape-doctor-profile-data-from-states-board-of-medicine-websites" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scrape Doctor Profile Data from State's Board of Medicine Websites</h3>

<p>an example of using <strong>selenium + lxml</strong> libs to extract table data from web pages.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>I'll use the Washington DC board of medicine website as an example: <a href="https://app.hpla.doh.dc.gov/Weblookup/">https://app.hpla.doh.dc.gov/Weblookup/</a></p>

<p>My goal is to scrape all the search results (multiple pages) from the .asp website and then save the data into a csv file.</p>

<p>First, let's set up our environment:</p>

<pre><code>$ pip install selenium
$ pip install lxml
</code></pre>

<p>Then we add the following libs in our script</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.support.ui import WebDriverWait
from lxml import html
import csv
</code></pre>

<h4>
<a id="loading-the-search-page" class="anchor" href="#loading-the-search-page" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Loading the search page</h4>

<pre><code>driver = webdriver.Firefox()
driver.get("https://app.hpla.doh.dc.gov/Weblookup/")
</code></pre>

<h4>
<a id="selecting-from-the-license-type-dropdown-menu" class="anchor" href="#selecting-from-the-license-type-dropdown-menu" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Selecting from the "License Type" dropdown menu</h4>

<p>If you look at the HTML associated with this dropdown menu, you will see the <code>name</code> that attributes to it is <code>t_web_lookup__license_type_name</code>
Next, select "Medicine and Surgery" from the dropdown and click the "Inspect Element", you will find the value associated with it is <code>"MEDICINE AND SURGERY"</code>.</p>

<p>Let's add these attributions in our script:</p>

<pre><code>dropdown = Select(driver.find_element_by_name("t_web_lookup__license_type_name"))
dropdown.select_by_value("MEDICINE AND SURGERY")
</code></pre>

<h4>
<a id="click-the-search-button" class="anchor" href="#click-the-search-button" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Click the Search button</h4>

<p>Like the dropdown, we need to find the HTML of the search button on the search page, here it is in the web inspector:</p>

<p>We can select this button and add a .click function in our script:</p>

<pre><code>search_button = driver.find_element_by_id("sch_button")
search_button.click()
</code></pre>

<p>Now when you run our script, a Firefox broswer will pop out and submit the form to the server.</p>

<h4>
<a id="extracting-the-search-results" class="anchor" href="#extracting-the-search-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Extracting the search results</h4>

<p>Let's create a function called <code>get_data()</code> and use the <code>html</code> function from <code>lxml</code> to extract the table data (ps: you can also use beautifulsoup if you like, I like lxml better because it is more straight forward.)</p>

<pre><code>def get_data(source):
    texts = []
    content = html.fromstring(source)
    rows - content.xpath(".//table[id@='datagrid_results']//tr")
    for row in rows:
        columns = row.xpath(".//td")
        text = [col.text_content() for col in columns]
        texts.append(text)
    return texts
</code></pre>

<p>This function will extract all the table dat on the current page, and put it in the list <code>texts</code>.</p>

<h4>
<a id="pagination" class="anchor" href="#pagination" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Pagination</h4>

<p>When you scoll down to the bottom of the page, you will notice that it only shows 40 pages at a time:</p>

<p>And when you click the "..." next to page 40, it will take to page 41 - 80, and so on so forth.
In order to get all the reuslts, we need to find the <code>xpath</code> of each page  number link, starting with page2:</p>

<pre><code>def go_to_next_page():
    next_page_link = driver.find_element_by_xpath("//table[@id='datagrid_results']//tr//td//span/following-sibling::a")
    next_page_link.click()
</code></pre>

<p>This function allows the script to go through all the <code>&lt;a&gt;</code>tags that contain the page number link.
Now we have all the stuff we need to extract data from this database, all we need to do write the data into a csv file:</p>

<pre><code>With open("dc.csv", "wb") as output:
    writer = csv.writer(output, delimiter=",")
    While True:
        data_from_page = get_data(driver.page_source)
        writer.writerows(data_from_page)
        try:
            go_to_next_page()
        except:
            break
driver.close()
</code></pre>

<h4>
<a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion</h4>

<p>That's all! Run the script on your computer and go get yourself a cup of coffee <img class="emoji" title=":coffee:" alt=":coffee:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/2615.png" height="20" width="20" align="absmiddle">
By the time you come back, the data will be ready for you in a csv file!</p>

<h4>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h4>

<p>If you have questions regarding the script or scraping in general, feel free to shoot me an email at <a href="mailto:wongsiutan@gmail.com">wongsiutan@gmail.com</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/siutanwong/Scrape_Doctor_Profile_Data">Scrape doctor profile data</a> is maintained by <a href="https://github.com/siutanwong">siutanwong</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
